{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in Large Language Models (LLMs)
",
    "**Thema:** Textgenerierung mit GPT-2  
",
    "**Autor:** [Dein Name]  
",
    "**Ziel:** Erste Experimente mit einem vortrainierten Large Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛠️ Setup
",
    "!pip install transformers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline
",
    "
",
    "# 📦 Modell laden
",
    "generator = pipeline("text-generation", model="gpt2")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📝 Erster Versuch
",
    "prompt = "Die Erde ist ein"
",
    "result = generator(prompt, max_length=20, num_return_sequences=1)
",
    "
",
    "print("🔮 Generierter Text:")
",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Interaktive Aufgabe
",
    "# Ändere den Prompt und max_length und sieh, wie sich der Output verändert.
",
    "
",
    "prompt = "Künstliche Intelligenz wird in Zukunft"
",
    "result = generator(prompt, max_length=30, num_return_sequences=2)
",
    "
",
    "print("\n🧪 Experiment:")
",
    "for i, r in enumerate(result):
",
    "    print(f"Version {i+1}:\n{r['generated_text']}\n")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 Mit Sampling spielen
",
    "# Temperatur: Steuerung der Kreativität (0.1 = konservativ, 1.0 = kreativ)
",
    "
",
    "prompt = "Ein Roboter betritt ein Klassenzimmer und"
",
    "result = generator(prompt, max_length=40, num_return_sequences=1, temperature=0.9, top_k=50)
",
    "
",
    "print("🎨 Mit Sampling-Parametern:")
",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Reflexion
",
    "- Wie kreativ oder sinnvoll ist der Text?
",
    "- Welche Rolle spielt der Prompt?
",
    "- Welche Probleme könnten bei realen Anwendungen auftreten (z. B. Bias, Halluzination)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Bonus: Eigene Modelle ausprobieren
",
    "Optional, wenn Zeit & GPU-Ressourcen verfügbar sind:
",
    "- `EleutherAI/gpt-neo-125M`
",
    "- `dbmdz/german-gpt2`
",
    "
",
    "```python
",
    "# generator = pipeline("text-generation", model="EleutherAI/gpt-neo-125M")
",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
